# Lab 02: Building LLM Orchestration Flows

### Estimated Duration: 60 minutes

## Overview

Participants will develop a **Standard Classification Flow** for categorizing data and a **Conversational RAG Flow** for enhancing AI conversations with contextually relevant information retrieval and response generation.

## Lab Objectives

After completing this lab, you will be able to complete the following tasks:

- Task 01: Create a standard classification flow
- Task 02: Create a conversational RAG flow

## Task 01: Create a Standard Classification Flow

1. In **Azure AI Studio**, navigate to **Tools > Prompt flow** and click on **+ Create**.

   ![](media/+create-prompt-flow.png)

1. In the flow creation window, select the **Standard flow** filter in the **Explore gallery** section and click on **Clone** under **Web Classification.**

   ![](media/web-classification-clone.png)

1. In the **Clone flow** window, accept the **default folder name value** and click on **Clone**.

    ![](media/web-clone-flow.png)

1. A standard flow will be created with the following structure:

    ![](media/web-classification-flow.png)

1. Notice that the flow has five nodes:

   - The first **fetch_text_content_from_url** is a Python node to extract the text from a web page.
   - Then, the content obtained by the extraction serves as input for an LLM node **summarize_text_content** to summarize the content.
   - The summarization, combined with the classification examples generated by a Python node **prepare_examples,** is the input for another LLM node **classify_with_llm,** where the classification is performed.
   - At the end, we have a Python node **convert_to_dict** responsible for formatting the output of the flow in a Python dictionary format.
  
1. Now that the flow has been created, we need a runtime to execute it in the **Prompt flow.** Select **Start Compute Session** to start a runtime to run your flow.

    ![](media/llm15.png)

   >**Note:** It will take 1-3 minutes to start the session.

1. After starting the runtime, we need to define the **Connection** with the LLM for each LLM step. In our case, these are **summarize_text_content** and **classify_with_llm**.

1. We will use the **Default_AzureOpenAI Connection** and **gpt-4** deployment, which connect to the Azure OpenAI resource that was created when the Azure AI project was set up.

   ![](media/web-summarize-text-content.png)

1. Associate the same connection for the **classify_with_llm** step.

   ![](media/web-classify-with-llm.png)

   >**Note:** You can leave the **response_format** field **blank** or select the **{"type":"text"}**.

1. Once the runtime is selected and the connections are configured, you can start the flow by clicking the **Run** button at the top of the page.

   ![](media/web-classification-run.png)

1. After finishing the execution, you will see that the flow is complete with all the steps.

   ![](media/web-flow-completed-steps.png)

1. You can view the result of the processing by clicking the last node.

## Task 02: Create a Conversational RAG Flow

1. In **Azure AI Studio**, navigate to **Tools > Prompt flow** and click on **+ Create**.

   ![](media/llm16.png)

1. In the **flow creation** window, select the **Chat flow** filter in the **Explore gallery** section and click on **Clone** for the **Multi-Round Q&A on Your Data**.

   ![](media/multi-round-chat-flow-clone.png)

1. In the **Clone flow** window, accept the default folder name value and click on **Clone**.

1. A chat flow will be created with the following structure:

   ![](media/multi-round-flow.png)

1. Select **Start compute session** to start a runtime to run your flow.

   ![](media/multi-round-start-compute.png)

   >**Note:** It will take 1-3 minutes to start the session.

1. Click on the **Save** button to save your flow.

   ![](media/multi-round-save.png)

1. Flow overview:

   - The first node, **modify_query_with_history**, produces a search query using the user's question and their previous interactions.
   - Next, in the **lookup** node, the flow uses the vector index to conduct a search within a vector store, which is where the RAG pattern retrieval step takes place.
   - Following the search process, the **generate_prompt_context** node consolidates the results into a string.
   - This string then serves as input for the **Prompt_variants** node, which formulates various prompts.
   - Finally, these prompts are used to generate the user's answer in the **chat_with_context** node.

1. Before you can start running your flow, a crucial step is to establish the search index for the retrieval stage. This search index will be provided by the Azure AI Search service.

1. Navigate to **Components > Indexes (1)** and click on **+ New index (2)** to create a new vector index.

   ![](media/+create-new-index.png)

1. On the **Source data** tab, select **Upload files** from the dropdown and upload the PDF **surface-pro-4-user-guide-EN.pdf** from your **C:\LabFiles** folder, and click on **Next**.

   ![](media/upload-files-indexes.png)

1. On the **Index settings** tab, select **Connect other Azure AI Search resource** from the **Select Azure AI Search Service** dropdown.

   ![](media/connect-ai-resource-indexes.png)

1. On the **Connect an existing resource** window, click on **Add connection**.

   ![](media/add-connection-indexes.png)

1. Back on the **Create an index** window, on the **Index settings** tab, accept the default values for name and auto-select VM, and click on **Next**.
   
   ![](media/auto-select-vm-indexes.png)

   >**Note:** The name of the indexer may differ.

1. On the **Search settings** tab, verify that the **Add vector search to this search resource** is selected and click on **Next**.

   >**Note:** The Azure OpenAI embedding model, **text-embedding-ada-002** (Version 2), will be deployed if not already.

   ![](media/add-vector-indexes.png)

1. On the **Review and finish** tab, review your settings and click on **Create**.

   >**Note:** The indexing job will be created and submitted for execution. It may take about 10 minutes from the time it enters the execution queue until it starts.

   ![](media/review-create-index.png)

1. Wait until the index status is shown as **Completed** before proceeding with the next steps.

   ![](media/index-complete-status.png)

1. Now, return to the RAG flow created in the **Prompt flow** to configure the **lookup** node.

1. After selecting the **lookup** node, click on the **mlindex_content** value.

   ![](media/lookup-node-mlindex.png)

1. On the **Generate** window, select **Registered Index** from the **index_type** dropdown, select your recently created vector index, and click on **Save**.

   ![](media/generate-index-save.png)

1. Back on the **lookup** node, select the **Hybrid (vector + keyword)** option from the **query_type** field.

   ![](media/lookup-node-query-type.png)

1. Now, let us define the connections of the nodes that link with LLM models. Select your default connection value from the dropdown and select **gpt-4** for the deployment name for both **modify_query_with_history** and **chat_with_context** nodes.

   ![](media/modify_query_with_history.png)

   ![](media/chat-with-context.png)

1. Click on **Save** after configuring the connections for the nodes.

1. Everything is now set up for you to initiate your chat flow. Simply click on the **blue chat** button located at the top right corner of your page to begin interacting with the flow.

   ![](media/multi-rag-flow-chat.png)
   
1. Ask questions in regards to the PDF in the chat box to get the relevant answers.

   ![](media/multi-rag-flow-chat-box.png)

## Summary

In this lab, you have performed  the following tasks:

- Created a standard classification flow.
- Created a conversational RAG flow.

### You have successfully completed the lab.
